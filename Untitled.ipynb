{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Annotation = namedtuple('Annotation', 'GOID start end source text')\n",
    "\n",
    "class Annotations(list):\n",
    "    def __repr__(self):\n",
    "        return 'Annotations<{} items>'.format(len(self))\n",
    "    \n",
    "    def to_csv(self, fp):\n",
    "        import csv  \n",
    "        with open(fp, 'w') as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerows(self)\n",
    "        \n",
    "    def to_json(self, fp):\n",
    "        import json\n",
    "        with open(fp, 'w') as f:\n",
    "            json.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "hamper = Annotations()\n",
    "\n",
    "for i in range(15):\n",
    "    GOID = 'GO:12345'\n",
    "    source = '998900'\n",
    "    text = 'text'\n",
    "    start = random.randint(0, 2000)\n",
    "    end = start + 10\n",
    "    a = Annotation(GOID, start, end, source, text)\n",
    "    hamper.append(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotations<15 items>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/text.csv', 'w') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(hamper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp = 'data/test.json'\n",
    "import json\n",
    "with open(fp, 'w') as f:\n",
    "    json.dump(hamper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sentence = namedtuple('Sentence', 'text offset ref')\n",
    "Grounds = namedtuple('Grounds', 'evidences sentence')\n",
    "Candidate = namedtuple('Candidate', 'statement density evidences grounds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GoData(dict):\n",
    "    def __init__(self, obo_path):\n",
    "        \n",
    "        self._read(obo_path)\n",
    "        \n",
    "        self.goid2mindepth = dict()\n",
    "        self.goid2maxdepth = dict()\n",
    "        self._calculate_depth()\n",
    "        \n",
    "        self.goid2above = dict()\n",
    "        self.goid2below = dict()\n",
    "        self.goid2density = dict()\n",
    "        self._calculate_density()\n",
    "        \n",
    "        self.biological_process = partial(self._get_namespace, 'biological_process')\n",
    "        self.cellular_component = partial(self._get_namespace, 'cellular_component')\n",
    "        self.molecular_function = partial(self._get_namespace, 'molecular_function')\n",
    "    \n",
    "    def _read(self, obo_path):\n",
    "        \"\"\"Read GO data from OBO file\"\"\"\n",
    "        \n",
    "        with open(obo_path) as f:\n",
    "            text = f.read()\n",
    "        blocks = text.split('\\n\\n')\n",
    "        term_blocks = filter(lambda block:block[0:6]=='[Term]', \n",
    "                             blocks)\n",
    "        \n",
    "        for term_block in term_blocks:\n",
    "            goid = None\n",
    "            name = None\n",
    "            namespace = None\n",
    "            synonym_list = list()\n",
    "            parent_list = list()\n",
    "            \n",
    "            if 'is_obsolete: true' in term_block:\n",
    "                continue\n",
    "            lines = term_block.split('\\n')\n",
    "            for line in lines[1:]:\n",
    "                key, sep, value = line.partition(':')\n",
    "                if key == 'id':\n",
    "                    goid = value.strip()\n",
    "                if key == 'name':\n",
    "                    name = value.strip()\n",
    "                if key == 'synonym':\n",
    "                    synotext = value.strip()\n",
    "                    synonym = re.findall(r'\"(.*?)\"', synotext)[0]\n",
    "                    synonym_list.append(synonym)\n",
    "                if key == 'namespace':\n",
    "                    namespace = value.strip()\n",
    "                if key == 'is_a':\n",
    "                    parent_id, sep , parent_name = value.partition('!')\n",
    "                    parent_list.append(parent_id.strip())\n",
    "                    \n",
    "            concept = Concept(goid, name, namespace, synonym_list, parent_list)\n",
    "            self[goid] = concept\n",
    "    \n",
    "    def _calculate_depth(self):\n",
    "        \n",
    "        cache = dict()\n",
    "        \n",
    "        def _calc_depth(goid, func):\n",
    "            if goid in {'GO:0003674', 'GO:0008150', 'GO:0005575'}:\n",
    "                return 1\n",
    "            try:\n",
    "                return cache[goid]\n",
    "            except KeyError:\n",
    "                concept = self[goid]\n",
    "                return func(_calc_depth(parent_id, func) for parent_id in concept.parent_list) + 1\n",
    "        \n",
    "        for goid in self.keys():\n",
    "            self.goid2maxdepth[goid] = _calc_depth(goid, max)\n",
    "            self.goid2mindepth[goid] = _calc_depth(goid, min)\n",
    "    \n",
    "    def _calculate_density(self):\n",
    "        \n",
    "        above_cache = self.goid2above\n",
    "        \n",
    "        def _above(goid):\n",
    "            if goid in {'GO:0003674', 'GO:0008150', 'GO:0005575'}:\n",
    "                return set()\n",
    "            try:\n",
    "                return above_cache[goid]\n",
    "            except KeyError:\n",
    "                concept = self[goid]\n",
    "                above = set.union(*[_above(parent_id) for parent_id in concept.parent_list])\n",
    "                above |= set(concept.parent_list)\n",
    "                above_cache[goid] = above\n",
    "                return above\n",
    "        \n",
    "        for goid in self.keys():\n",
    "            above_cache[goid] = _above(goid)\n",
    "        \n",
    "        below_cache = defaultdict(set)\n",
    "        \n",
    "        for goid, above in above_cache.items():\n",
    "            for parent_id in above:\n",
    "                below_cache[parent_id].add(goid)\n",
    "        \n",
    "        self.goid2below = below_cache\n",
    "        \n",
    "        total = len(self)\n",
    "        for goid in self.keys():\n",
    "            below = self.goid2below.get(goid, set())\n",
    "            self.goid2density[goid] = float(len(below) + 1) / total\n",
    "        \n",
    "        for concept in self.values():\n",
    "            goid = concept.goid\n",
    "            concept.density = self.goid2density[goid]\n",
    "\n",
    "    def _get_namespace(self, namespace):\n",
    "        for goid, concept in self.items():\n",
    "            if concept.namespace == namespace:\n",
    "                yield concept\n",
    "    \n",
    "\n",
    "class Concept(object):\n",
    "    def __init__(self, goid, name, namespace, synonym_list, parent_list, density=-1):\n",
    "        self.goid = goid\n",
    "        self.name = name\n",
    "        self.namespace = namespace\n",
    "        self.ns = {'biological_process': 'BP', \n",
    "                  'cellular_component': 'CC', \n",
    "                  'molecular_function': 'MF'}[self.namespace]\n",
    "        self.synonym_list = synonym_list\n",
    "        self.labels = [name] + synonym_list\n",
    "        self.parent_list = parent_list\n",
    "        self.statements = [name] + self.synonym_list\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'Concept<{} {} {}>'.format(self.goid, self.ns, self.name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obo_path = 'data/craft-1.0/ontologies/GO.obo'\n",
    "gd = GoData(obo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Term = namedtuple('Term', 'lemma ref')\n",
    "            \n",
    "class Entity(Term):\n",
    "    pass\n",
    "\n",
    "class Pattern(Term):\n",
    "    pass\n",
    "\n",
    "class Constraint(Term):\n",
    "    pass\n",
    "    \n",
    "class Evidence(namedtuple('Evidence', 'term text start end')):\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        Return distance between the start of this term to the end of another term\n",
    "        Notice: abs(a - b) != abs(b - a)\n",
    "        \"\"\"\n",
    "        return self.start - other.end\n",
    "\n",
    "class Statement(namedtuple('Statement', 'statid evidences')):\n",
    "    \"\"\"\n",
    "    A Statement is a collection of evidences\n",
    "    \"\"\"\n",
    "    def __eq__(self, other):\n",
    "        try:\n",
    "            self.eq_terms(other)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash('statement:' + self.statid)\n",
    "    \n",
    "    def terms(self):\n",
    "        return [evidence.term for evidence in self.evidences]\n",
    "        \n",
    "    def eq_terms(self, other):\n",
    "        terms = []\n",
    "        if len(self.evidences) != len(other.evidences):\n",
    "            raise ValueError('The two statements have different lengths of evidences')\n",
    "        for this_evidence, other_evidence in zip(self.evidences, other.evidences):\n",
    "            this_term, other_term = this_evidence.term, other_evidence.term\n",
    "            if type(this_term) != type(other_term):\n",
    "                raise ValueError('The two statements have different type sequences of terms')\n",
    "            elif all([isinstance(this_term, Pattern),\n",
    "                      not this_term == other_term]):\n",
    "                raise ValueError('The two statements has different patterns')\n",
    "            elif not any([isinstance(this_term, Pattern), \n",
    "                          this_term == other_term]):\n",
    "                terms.append((other_term, this_term))\n",
    "        return terms\n",
    "    \n",
    "    \n",
    "class Cluster(object):\n",
    "    def __init__(self, primary_term, terms=None):\n",
    "        self.primary_term = primary_term\n",
    "        if terms is None:\n",
    "            self.terms = set()\n",
    "        else:\n",
    "            self.terms = set(terms)\n",
    "            \n",
    "        self.updated_fragments = False\n",
    "        self._fragments = set()\n",
    "        self._term_queue = list(self.terms)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.primary_term)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Cluster({})<{} terms>\".format(repr(self.primary_term), len(self.terms))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for term in self.terms:\n",
    "            yield term\n",
    "    \n",
    "    def fragments(self):\n",
    "        if self.updated_fragments:\n",
    "            return self._fragments\n",
    "        \n",
    "        else:\n",
    "            for term in self._term_queue:\n",
    "                self._fragments |= set(term.token.split(' '))\n",
    "            self._term_queue = []\n",
    "            self.updated_fragments = True\n",
    "            return self._fragments\n",
    "            \n",
    "    def add(self, term):\n",
    "        self.updated_fragments = False\n",
    "        self._term_queue.append(term)\n",
    "        self.terms.add(term)\n",
    "    \n",
    "    def merge(self, other):\n",
    "        self.updated_fragments = False\n",
    "        self.terms |= other.terms\n",
    "        self._fragments |= other.fragments()\n",
    "        del other\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClusterBook(object):\n",
    "    def __init__(self):\n",
    "        self.clusters = set()\n",
    "        self.index = dict()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'ClusterBook <{} clusters, {} terms>'.format(len(self.clusters), len(self.index.keys()))\n",
    "    \n",
    "    def add(self, cluster):\n",
    "        self.clusters.add(cluster)\n",
    "        for term in cluster:\n",
    "            self.index[term] = cluster\n",
    "    \n",
    "    def merge(self, cluster1, cluster2):\n",
    "        if cluster1 in self.clusters:\n",
    "            for term in cluster2:\n",
    "                self.index[term] = cluster1\n",
    "            cluster1.merge(cluster2)\n",
    "            \n",
    "        elif cluster2 in self.clusters:\n",
    "            for term in cluster1:\n",
    "                self.index[term] = cluster2\n",
    "            cluster2.merge(cluster1)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    \n",
    "    def merge_term(self, term1, term2):\n",
    "        cluster = self.index[term1]\n",
    "        cluster.add(term2)\n",
    "        self.index[term2] = cluster\n",
    "        \n",
    "def has_common(cluster1, cluster2):\n",
    "    set1 = set([t.lemma for t in cluster1.terms])\n",
    "    set2 = set([t.lemma for t in cluster2.terms])\n",
    "    if len(set1 & set2) == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def jaccard(cluster1, cluster2):\n",
    "    set1 = cluster1.fragments()\n",
    "    set2 = cluster2.fragments()\n",
    "    return len(set1 & set2)/len(set1 | set2)\n",
    "\n",
    "def sim(cluster1, cluster2):\n",
    "    if not has_common(cluster1, cluster2):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return jaccard(cluster1, cluster2)\n",
    "        \n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "        self.pmid_set = set([s.ref for s in sentences])\n",
    "        self.bins = {i:set() for i in range(10)}\n",
    "        pmid_list = sorted(list(self.pmid_set))\n",
    "        random.seed(0)\n",
    "        random.shuffle(pmid_list)\n",
    "        for i, pmid in enumerate(pmid_list):\n",
    "            b = i % 10\n",
    "            self.bins[b].add(pmid)\n",
    "            \n",
    "    def get_training_testing(self, bin_number):\n",
    "        \"\"\"The bin_number should in range(10)\"\"\"\n",
    "        training = []\n",
    "        testing = []\n",
    "        for sentence in self.sentences:\n",
    "            if sentence.docid not in self.bins[bin_number]:\n",
    "                training.append(sentence)\n",
    "            else:\n",
    "                testing.append(sentence)\n",
    "        return training, testing\n",
    "\n",
    "class TermIndex(defaultdict):\n",
    "    def __add__(self, other):\n",
    "        result = TermIndex(set)\n",
    "        for key in self.keys() | other.keys():\n",
    "            result[key] = self[key] | other[key]\n",
    "        return result\n",
    "        \n",
    "def _fit_border(text, span):\n",
    "    start, end = span\n",
    "    left_border = text[max(0, start-1):start+1]\n",
    "    right_border = text[end-1:end+1]\n",
    "    judge = re.compile(r'(.\\b.|^.$)').match\n",
    "    return all([judge(left_border),\n",
    "                judge(right_border)])\n",
    "\n",
    "class SolidExtractor(object):\n",
    "    def __init__(self, text2primary_terms):\n",
    "        self.text2primary_terms = text2primary_terms\n",
    "        \n",
    "        builder = AcoraBuilder()\n",
    "        for text in text2primary_terms:\n",
    "            builder.add(text)\n",
    "        self.ac = builder.build()\n",
    "    \n",
    "    def findall(self, sentence):\n",
    "        ac = self.ac\n",
    "        text2primary_terms = self.text2primary_terms\n",
    "        result = []\n",
    "        offset = sentence.offset\n",
    "        for text, raw_start in ac.findall(sentence.text):\n",
    "            for primary_term in text2primary_terms[text]:\n",
    "                start = raw_start + offset\n",
    "                raw_end = raw_start + len(text)\n",
    "                end = start + len(text)\n",
    "                if _fit_border(sentence.text, (raw_start, raw_end)):\n",
    "                    if isinstance(primary_term, Entity):\n",
    "                        Class_ = Entity\n",
    "                    elif isinstance(primary_term, Constraint):\n",
    "                        Class_ = Constraint\n",
    "                    else:\n",
    "                        continue\n",
    "                    lemma = primary_term.lemma\n",
    "                    ref = primary_term.ref\n",
    "                    term = Class_(lemma, ref)\n",
    "                    evidence = Evidence(term, text, start, end)\n",
    "                    result.append(evidence)\n",
    "        return result\n",
    "                    \n",
    "class SoftExtractor(object):\n",
    "    def __init__(self, pattern_regex):\n",
    "        self.pattern_ex = re.compile(pattern_regex)\n",
    "    \n",
    "    def findall(self, sentence):\n",
    "        ex = self.pattern_ex\n",
    "        offset = sentence.offset\n",
    "        result = []\n",
    "        for m in ex.finditer(sentence.text):\n",
    "            lemma = list(filter(lambda item: item[1] is not None, m.groupdict().items()))[0][0]\n",
    "            raw_start, raw_end = m.span()\n",
    "            text = sentence.text[raw_start:raw_end]\n",
    "            start, end = raw_start + offset, raw_end + offset\n",
    "            term = Pattern(lemma, 'annotator')\n",
    "            evidence = Evidence(term, text, start, end)\n",
    "            result.append(evidence)\n",
    "        return result\n",
    "\n",
    "class JoinExtractor(object):\n",
    "    def __init__(self, extractors):\n",
    "        self.extractors = extractors\n",
    "        \n",
    "    def findall(self, sentence):\n",
    "        result = []\n",
    "        for extractor in self.extractors:\n",
    "            result.extend(extractor.findall(sentence))\n",
    "        result.sort(key=lambda e: e.start)\n",
    "        return result\n",
    "\n",
    "def gather_evidences(statement, index, term2index_evidence):\n",
    "    evidences = []\n",
    "    for term in statement.terms():\n",
    "        try:\n",
    "            i_evidence = [(abs(index - i), evidence) for i, evidence in term2index_evidence[term]]\n",
    "            evidence = sorted(i_evidence, key=lambda x:x[0])\n",
    "            evidences.append(evidence[0][1])\n",
    "        except:\n",
    "            pass\n",
    "    return evidences\n",
    "        \n",
    "def gather_baskets(grounds, more_info, term2statements):\n",
    "    baskets = []\n",
    "    term2index_evidence = defaultdict(list)\n",
    "    queue = []\n",
    "    evidences = grounds.evidences\n",
    "    for i, evidence in enumerate(evidences):\n",
    "        term = evidence.term\n",
    "        term2index_evidence[term].append((i, evidence))\n",
    "        if isinstance(term, Entity) or isinstance(term, Pattern):\n",
    "            queue.append((i, term))\n",
    "    term2index_evidence = dict(term2index_evidence)\n",
    "    for i, ep in queue:\n",
    "        candidates = []\n",
    "        statements = term2statements.get(ep, set())\n",
    "\n",
    "        for statement in statements:\n",
    "            statid = statement.statid\n",
    "            goid = statid.partition('%')[0]\n",
    "            density = gd.goid2density[goid]\n",
    "            evidences = gather_evidences(statement, i, term2index_evidence)\n",
    "            candidate = Candidate(statement, \n",
    "                                  density, \n",
    "                                  gather_evidences(statement, i, term2index_evidence),\n",
    "                                  grounds)\n",
    "            candidates.append(candidate)\n",
    "        candidates.sort(key=lambda c: c.density, reverse=True)\n",
    "        baskets.append(Basket(candidates, more_info))\n",
    "    return baskets\n",
    "    \n",
    "def concept_features(candidate):\n",
    "    statement = candidate.statement\n",
    "    statid = statement.statid\n",
    "    goid, sep, sulfix = statid.partition('%')\n",
    "    namespace = gd[goid].namespace\n",
    "    features = {'GOID': goid,\n",
    "                'STATID': statid,\n",
    "                'NAMESPACE': namespace}\n",
    "    return features\n",
    "    \n",
    "def evidence_features(candidate):\n",
    "    evidences = candidate.evidences\n",
    "    sentence_text = candidate.grounds.sentence.text\n",
    "    offset = candidate.grounds.sentence.offset\n",
    "    starts = [e.start for e in evidences]\n",
    "    ends = [e.end for e in evidences]\n",
    "    raw_start = min(starts) -  offset\n",
    "    raw_end = max(ends) - offset\n",
    "    length = raw_end - raw_start\n",
    "    text = sentence_text[raw_start:raw_end]\n",
    "    features = {'LENGTH': length,\n",
    "                'TEXT': text,\n",
    "                'TEXT[:3]': text[:3],\n",
    "                'TEXT[-3:]': text[-3:]}\n",
    "    return features\n",
    "    \n",
    "def bias_features(candidate):\n",
    "    features = dict()\n",
    "    statement = candidate.statement\n",
    "    evidences = candidate.evidences\n",
    "    terms_in_evidences = set([e.term for e in evidences])\n",
    "    features['OMIT'] = [term.lemma for term in statement.terms() if term not in terms_in_evidences]\n",
    "    features['SATURATION'] = len(evidences) / len(statement.evidences)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def candidate2features(candidate):\n",
    "    return dict(ChainMap(concept_features(candidate), evidence_features(candidate), bias_features(candidate)))\n",
    "    \n",
    "def grounds2features(grounds):\n",
    "    evidences = grounds.evidences\n",
    "    features = {#'GROUNDS': {e.text for e in evidences},\n",
    "                #'LEMMA': {e.term.lemma for e in evidences}\n",
    "                }\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def sentence2baskets(sentence, extractor, term2statements):\n",
    "    grounds = Grounds(extractor.findall(sentence), sentence)\n",
    "    more_info = grounds2features(grounds)\n",
    "    more_info.update({'PMID': sentence.docid})\n",
    "    baskets = gather_baskets(grounds, more_info, term2statements)\n",
    "    return baskets\n",
    "    \n",
    "def join_baskets(sentences, extractor, term2statements):\n",
    "    all_baskets = []\n",
    "    for sentence in sentences:\n",
    "        baskets = sentence2baskets(sentence, extractor, term2statements)\n",
    "        all_baskets.extend(baskets)\n",
    "    return all_baskets\n",
    "    \n",
    "    \n",
    "def basket2features(basket):\n",
    "    candidate_features = []\n",
    "    for candidate in basket.candidates:\n",
    "        features = candidate2features(candidate)\n",
    "        features.update(basket.more_info)\n",
    "        candidate_features.append(features)\n",
    "        \n",
    "    return candidate_features\n",
    "    \n",
    "    \"\"\"\n",
    "    final_features = []\n",
    "    for i, features in enumerate(candidate_features):\n",
    "        features.update({'SOB': False,\n",
    "                         'EOB': False,\n",
    "                         'DENSITY[i-1]': False,\n",
    "                         'DENSITY[i+1]': False,\n",
    "                         'SATURATION[i-1]': False,\n",
    "                         'SATURATION[i+1]': False})\n",
    "        if i == 0:\n",
    "            features.update({'SOB': True})\n",
    "        else:\n",
    "            last_density = candidate_features[i-1]['DENSITY']\n",
    "            last_saturation = candidate_features[i-1]['SATURATION']\n",
    "            features.update({'DENSITY[i-1]': last_density,\n",
    "                             'SATURATION[i-1]': last_saturation})\n",
    "        if i == len(candidate_features) - 1:\n",
    "            features.update({'EOB': True})\n",
    "        else:\n",
    "            next_density = candidate_features[i+1]['DENSITY']\n",
    "            next_saturation = candidate_features[i+1]['SATURATION']\n",
    "            features.update({'DENSITY[i+1]': next_density,\n",
    "                             'SATURATION[i+1]': next_saturation})\n",
    "        final_features.append(features)\n",
    "    return final_features\"\"\"\n",
    "    \n",
    "def baskets2X(baskets):\n",
    "    X = []\n",
    "    for basket in baskets:\n",
    "        X.append(basket2features(basket))\n",
    "    return X\n",
    "    \n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "def goldstandard_from_xml(fpath):\n",
    "    pmid = re.findall(r'/(\\d+)\\.txt\\.knowtator\\.xml', fpath)[0]\n",
    "    assert pmid.isdigit\n",
    "    t = etree.parse(fpath)\n",
    "    mentionid2goid = dict()\n",
    "    mentionid2span = dict()\n",
    "    for classmention in t.xpath('//classMention'):\n",
    "        mentionid = ''.join(classmention.xpath('@id'))\n",
    "        goid = ''.join(classmention.xpath('mentionClass/@id'))\n",
    "        mentionid2goid[mentionid] = goid\n",
    "\n",
    "    for annotation in t.xpath('//annotation'):\n",
    "        mentionid = ''.join(annotation.xpath('mention/@id'))\n",
    "        spannedtext = ''.join(annotation.xpath('spannedText/text()'))\n",
    "        start = int(''.join(annotation.xpath('span/@start')))\n",
    "        end = int(''.join(annotation.xpath('span/@end')))\n",
    "        mentionid2span[mentionid] = (start, end, spannedtext)\n",
    "\n",
    "    key_left = mentionid2goid.keys()\n",
    "    key_right = mentionid2span.keys()\n",
    "    assert key_left == key_right\n",
    "\n",
    "    goldstandard = []\n",
    "    for mentionid in key_left:\n",
    "        goid = mentionid2goid[mentionid]\n",
    "        start, end, spannedtext = mentionid2span[mentionid]\n",
    "        goldstandard.append((pmid, goid, start, end, spannedtext))\n",
    "        \n",
    "    return goldstandard\n",
    "    \n",
    "from intervaltree import Interval, IntervalTree\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_forest(gold):\n",
    "    forest = defaultdict(IntervalTree)\n",
    "    for pmid, goid, start, end, spannedtext in gold:\n",
    "        t = forest[pmid]\n",
    "        t[start:end] = (goid, spannedtext)\n",
    "    forest = dict(forest)\n",
    "    return forest\n",
    "\n",
    "\n",
    "def basket2labels(basket, forest):\n",
    "    labels = []\n",
    "    pmid = basket.more_info['PMID']\n",
    "    for candidate in basket.candidates:\n",
    "        statid = candidate.statement.statid\n",
    "        evidences = candidate.evidences\n",
    "        goid = statid.partition('%')[0]\n",
    "        starts = [e.start for e in evidences]\n",
    "        ends = [e.end for e in evidences]\n",
    "        start = min(starts)\n",
    "        end = max(ends)\n",
    "        span = (start, end)\n",
    "        gold_goids = {iv.data[0] for iv in forest[pmid][slice(*span)]}\n",
    "        if goid in gold_goids:\n",
    "            labels.append('BINGO')\n",
    "        else:\n",
    "            labels.append('NOT')\n",
    "            \n",
    "    return labels      \n",
    "    \n",
    "    \n",
    "def baskets2y(baskets, forest):\n",
    "    y = []\n",
    "    for basket in baskets:\n",
    "        y.append(basket2labels(basket, forest))\n",
    "    return y\n",
    "    \n",
    "def candidate2result(candidate):\n",
    "    pmid = candidate.grounds.sentence.docid\n",
    "    goid = candidate.statement.statid.partition('%')[0]\n",
    "    start = min([e.start for e in candidate.evidences])\n",
    "    end = max([e.end for e in candidate.evidences])\n",
    "    raw_start = start - candidate.grounds.sentence.offset\n",
    "    raw_end = end - candidate.grounds.sentence.offset\n",
    "    text = candidate.grounds.sentence.text[raw_start:raw_end]\n",
    "    return (pmid, goid, start, end, text)\n",
    "    \n",
    "class Report(object):\n",
    "    def __init__(self, tp, fp, fn, message):\n",
    "        self.tp = tp\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "        self.message = message\n",
    "    \n",
    "    def recall(self):\n",
    "        return len(self.tp) / len(self.tp | self.fn)\n",
    "    \n",
    "    def precision(self):\n",
    "        return len(self.tp) / len(self.tp | self.fp)\n",
    "    \n",
    "    def f1(self):\n",
    "        r = self.recall()\n",
    "        p = self.precision()\n",
    "        return 2 * r * p / (r + p)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        r = self.recall()\n",
    "        p = self.precision()\n",
    "        f = self.f1()\n",
    "        syntax = 'Report<R{r:.2%} P{p:.2%} F{f:.2%} {m!r}>'\n",
    "        return syntax.format(r=r, p=p, f=f, m=self.message)\n",
    "    \n",
    "def evaluate(system, goldstandard, message):\n",
    "    slim_system = {i[:4] for i in system}\n",
    "    slim_goldstandard = {i[:4] for i in goldstandard}\n",
    "    slim2gold = ChainMap({i[:4]: i for i in goldstandard}, \n",
    "                         {i[:4]: i for i in system})\n",
    "    slim_tp = slim_system & slim_goldstandard\n",
    "    slim_fp = slim_system - slim_goldstandard\n",
    "    slim_fn = slim_goldstandard - slim_system\n",
    "    tp = {slim2gold[i] for i in slim_tp}\n",
    "    fp = {slim2gold[i] for i in slim_fp}\n",
    "    fn = {slim2gold[i] for i in slim_fn}\n",
    "    return Report(tp, fp, fn, message)\n",
    "\n",
    "def _flaten_X(X):\n",
    "    output = []\n",
    "    for features_list in X:\n",
    "        for features in features_list:\n",
    "            new_features = {}\n",
    "            for key, value in features.items():\n",
    "                if isinstance(value, set) or isinstance(value, list):\n",
    "                    new_features.update({'{}={}'.format(key, v): True for v in value})\n",
    "                elif isinstance(value, str):\n",
    "                    new_features.update({'{}={}'.format(key, value): True})\n",
    "                else:\n",
    "                    new_features.update({key: value})\n",
    "            output.append(new_features)\n",
    "    return output\n",
    "\n",
    "def _flaten_y(y):\n",
    "    new_y = []\n",
    "    for label in [label for list_of_label in y for label in list_of_label]:\n",
    "        if label == 'NOT':\n",
    "            new_y.append(0)\n",
    "        else:\n",
    "            new_y.append(1)\n",
    "    return new_y\n",
    "\n",
    "def flaten(X, y):\n",
    "    return _flaten_X(X), _flaten_y(y)\n",
    "\n",
    "import copy\n",
    "def remove_keys(list_of_features, feature_names):\n",
    "    output = copy.deepcopy(list_of_features)\n",
    "    for feature in output:\n",
    "        for name in feature_names:\n",
    "            del feature[name]\n",
    "    return output\n",
    "\n",
    "def feature_remove_filter(X, feature_names):\n",
    "    out_X = []\n",
    "    for list_of_features in X:\n",
    "        list_of_features = remove_keys(list_of_features, feature_names)\n",
    "        out_X.append(list_of_features)\n",
    "    return out_X\n",
    "\n",
    "\n",
    "def candidate2result(candidate):\n",
    "    try:\n",
    "        pmid = candidate.grounds.sentence.docid\n",
    "    except: \n",
    "        pmid = candidate.sentence.docid\n",
    "    goid = candidate.statement.statid.partition('%')[0]\n",
    "    start = min([e.start for e in candidate.evidences])\n",
    "    end = max([e.end for e in candidate.evidences])\n",
    "    try:\n",
    "        raw_start = start - candidate.grounds.sentence.offset\n",
    "        raw_end = end - candidate.grounds.sentence.offset\n",
    "        text = candidate.grounds.sentence.text[raw_start:raw_end]\n",
    "    except:\n",
    "        raw_start = start - candidate.sentence.offset\n",
    "        raw_end = end - candidate.sentence.offset\n",
    "        text = candidate.sentence.text[raw_start:raw_end]\n",
    "    return (pmid, goid, start, end, text)\n",
    "\n",
    "def recover(all_baskets, y):\n",
    "    results = []\n",
    "    for basket, labels in zip(all_baskets, y):\n",
    "        basket_results = set()\n",
    "        pmid = basket.more_info['PMID']\n",
    "        for candidate, label in zip(basket.candidates, labels):\n",
    "            if label == 1 or label == 'BINGO':\n",
    "                 basket_results.add(candidate2result(candidate))\n",
    "        sorted_basket_results = sorted(list(basket_results), key=lambda r:r[1])\n",
    "        results.extend(sorted_basket_results)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obo_path = 'data/craft-1.0/ontologies/GO.obo'\n",
    "gd = GoData(obo_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
